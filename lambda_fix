import boto3
import csv
import io
from datetime import datetime, timedelta
from typing import List, Dict, Any

s3 = boto3.client("s3")
sns = boto3.client("sns")

BUCKET = "bucketname"
BASE_PREFIX = ""
SNS_TOPIC_ARN = "arn:aws:sns:us-east-1:123456789012:weekly-report"

CADENCE_PREFIX = ""
SNS_MAX = 260000


def monday_two_weeks_ago(today: datetime) -> str:
    """Monday-based cadence: Monday two weeks ago."""
    monday_this_week = today - timedelta(days=today.weekday())
    return (monday_this_week - timedelta(days=14)).strftime("%Y-%m-%d")


def exact_days_ago(today: datetime, days: int = 14) -> str:
    """Calendar-based: exactly N days ago."""
    return (today - timedelta(days=days)).strftime("%Y-%m-%d")


def resolve_start_date(event: Dict[str, Any]) -> str:
    """
    Behavior:
      - Scheduled Monday run (default on Monday UTC): Monday 2 weeks ago
      - Manual run any other day (default): exactly 14 days ago
    Overrides:
      - {"start_date": "YYYY-MM-DD"}
      - {"mode": "weekly"} forces Monday cadence
      - {"days_ago": 14} forces exact N days
    """
    today = datetime.utcnow()
    event = event or {}

    if event.get("start_date"):
        return event["start_date"]

    if event.get("mode") == "weekly":
        return monday_two_weeks_ago(today)

    if event.get("days_ago") is not None:
        return exact_days_ago(today, int(event["days_ago"]))

    # default behavior
    if today.weekday() == 0:  # Monday (UTC)
        return monday_two_weeks_ago(today)

    return exact_days_ago(today, 14)


def lambda_handler(event, context):
    start_date = resolve_start_date(event or {})
    print(f"[INFO] Using start_date={start_date}")

    agencies = list_child_prefixes(BASE_PREFIX)  # full prefixes
    print(f"[INFO] Found agencies={len(agencies)}")

    sections: List[str] = []

    for agency_prefix in agencies:
        agency_name = agency_prefix.rstrip("/").split("/")[-1]  # e.g., agency=wholesales
        print(f"[INFO] Agency={agency_name}")

        # bypass folders (bypass-DNS_53, bypass-DoH, bypass-DoT, etc.)
        bypass_prefixes = list_child_prefixes(agency_prefix)
        print(f"[INFO]  bypass_prefixes={len(bypass_prefixes)}")

        for bypass_prefix in bypass_prefixes:
            bypass_name = bypass_prefix[len(agency_prefix):].rstrip("/")  # relative label
            # ipv folders (ipv=IPv4, ipv=IPv6)
            ipv_prefixes = list_child_prefixes(bypass_prefix)

            for ipv_prefix in ipv_prefixes:
                ipv_name = ipv_prefix[len(bypass_prefix):].rstrip("/")  # e.g., ipv=IPv4

                # ip_field folders (ip_field=DIPS, ip_field=SIPS)
                ip_field_prefixes = list_child_prefixes(ipv_prefix)

                for ipf_prefix in ip_field_prefixes:
                    ipf_name = ipf_prefix[len(ipv_prefix):].rstrip("/")  # e.g., ip_field=DIPS

                    # FINAL target prefix exactly matching your example key:
                    # .../agency=.../bypass-.../ipv=.../ip_field=.../cadence=week/start_date=YYYY-MM-DD/
                    target_prefix = f"{ipf_prefix}{CADENCE_PREFIX}start_date={start_date}/"
                    print(f"[DEBUG] Checking: {target_prefix}")

                    rows = read_csvs(target_prefix)

                    if rows:
                        header = (
                            f"\n===== {agency_name} | {bypass_name} | {ipv_name} | {ipf_name} "
                            f"| start_date={start_date} =====\n"
                        )
                        sections.append(header + "\n".join(rows))

    if not sections:
        publish_sns(
            subject=f"DNS/DoH/DoT Report (start_date={start_date})",
            message=(
                f"No CSV data found for start_date={start_date} under {BASE_PREFIX}\n"
                f"Tip: verify the folder exists exactly: .../{CADENCE_PREFIX}start_date={start_date}/"
            )
        )
        return {"status": "no_data", "start_date": start_date}

    publish_sns_chunked(
        subject=f"DNS/DoH/DoT Report (start_date={start_date})",
        message="\n\n".join(sections)
    )
    return {"status": "ok", "start_date": start_date, "sections": len(sections)}


def list_child_prefixes(parent_prefix: str) -> List[str]:
    """
    Lists immediate child "folders" as full prefixes using Delimiter="/".
    Returns list like: [".../agency=wholesales/", ".../agency=foo/"]
    """
    prefixes: List[str] = []
    paginator = s3.get_paginator("list_objects_v2")

    for page in paginator.paginate(Bucket=BUCKET, Prefix=parent_prefix, Delimiter="/"):
        for cp in page.get("CommonPrefixes", []):
            prefixes.append(cp["Prefix"])

    return prefixes


def read_csvs(prefix: str) -> List[str]:
    """
    Reads all .csv files under prefix and returns their CSV rows as strings.
    """
    paginator = s3.get_paginator("list_objects_v2")
    output: List[str] = []
    found_any = False

    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
        for obj in page.get("Contents", []):
            found_any = True
            key = obj["Key"]
            if key.lower().endswith(".csv"):
                print(f"[INFO] Found CSV: {key}")
                body = s3.get_object(Bucket=BUCKET, Key=key)["Body"].read().decode("utf-8", errors="replace")
                reader = csv.reader(io.StringIO(body))
                for row in reader:
                    output.append(", ".join(row))

    if not output:
        if not found_any:
            print(f"[DEBUG] No objects under prefix: {prefix}")
        else:
            print(f"[DEBUG] Objects exist but no .csv matched under prefix: {prefix}")

    return output


def publish_sns(subject: str, message: str):
    sns.publish(
        TopicArn=SNS_TOPIC_ARN,
        Subject=subject,
        Message=message[:SNS_MAX]
    )


def publish_sns_chunked(subject: str, message: str):
    """
    Split into multiple SNS messages if payload is too large.
    """
    if len(message) <= SNS_MAX:
        publish_sns(subject, message)
        return

    start = 0
    parts: List[str] = []
    while start < len(message):
        end = min(start + SNS_MAX, len(message))
        parts.append(message[start:end])
        start = end

    for i, chunk in enumerate(parts, start=1):
        sns.publish(
            TopicArn=SNS_TOPIC_ARN,
            Subject=f"{subject} (part {i}/{len(parts)})",
            Message=chunk
        )
