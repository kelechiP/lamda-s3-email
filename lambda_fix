import boto3
import csv
import io
from datetime import datetime, timedelta
from typing import List, Dict, Any

s3 = boto3.client("s3")
sns = boto3.client("sns")

BUCKET = "bucketname"
BASE_PREFIX = ""
SNS_TOPIC_ARN = "arn:aws:sns:us-east-1:123456789012:weekly-report"



# IMPORTANT: your actual folder is cadence=week (NOT cadence-week)
CADENCE_PREFIX = ""

SNS_MAX = 260000  # SNS payload safety (approx)


def monday_two_weeks_ago(today: datetime) -> str:
    """Return YYYY-MM-DD for Monday two weeks ago (week cadence)."""
    monday_this_week = today - timedelta(days=today.weekday())  # Monday 00:00-ish
    two_weeks_ago = monday_this_week - timedelta(days=14)
    return two_weeks_ago.strftime("%Y-%m-%d")


def exact_days_ago(today: datetime, days: int = 14) -> str:
    """Return YYYY-MM-DD for exactly N days ago."""
    return (today - timedelta(days=days)).strftime("%Y-%m-%d")


def resolve_start_date(event: Dict[str, Any]) -> str:
    """
    Resolution order:
      1) event["start_date"] explicit override (YYYY-MM-DD)
      2) event["mode"] == "weekly" -> Monday two weeks ago
      3) event["days_ago"] -> exact N days ago
      4) default:
            - if it's Monday (UTC) -> weekly cadence (Monday two weeks ago)
            - else -> exact 14 days ago
    """
    today = datetime.utcnow()

    if isinstance(event, dict) and event.get("start_date"):
        return event["start_date"]

    mode = (event.get("mode") if isinstance(event, dict) else None)

    if mode == "weekly":
        return monday_two_weeks_ago(today)

    if isinstance(event, dict) and event.get("days_ago") is not None:
        return exact_days_ago(today, int(event["days_ago"]))

    # default behavior:
    if today.weekday() == 0:  # Monday in UTC
        return monday_two_weeks_ago(today)

    return exact_days_ago(today, 14)


def lambda_handler(event, context):
    event = event or {}
    start_date = resolve_start_date(event)
    print(f"[INFO] Using start_date={start_date}")
    print(f"[INFO] BASE_PREFIX={BASE_PREFIX}")

    agencies = list_agency_folders()
    print(f"[INFO] Found agencies={len(agencies)}")

    report_sections: List[str] = []

    for agency in agencies:
        for folder in FOLDER_PATTERNS:
            prefix = (
                f"{BASE_PREFIX}{agency}/"
                f"{folder}"
                f"{CADENCE_PREFIX}start_date={start_date}/"
            )

            print(f"[DEBUG] Checking prefix: {prefix}")

            rows = read_csvs(prefix)
            if rows:
                section_header = f"\n===== {agency} | {folder} | start_date={start_date} =====\n"
                report_sections.append(section_header + "\n".join(rows))

    if not report_sections:
        publish_sns(
            subject=f"DNS/DoH/DoT Report (start_date={start_date})",
            message=f"No CSV data found for start_date={start_date} under {BASE_PREFIX}"
        )
        return {"status": "no_data", "start_date": start_date}

    full_message = "\n\n".join(report_sections)
    publish_sns_chunked(
        subject=f"DNS/DoH/DoT Report (start_date={start_date})",
        message=full_message
    )
    return {"status": "ok", "start_date": start_date, "sections": len(report_sections)}


def list_agency_folders() -> List[str]:
    """
    Agencies are immediate child folders under BASE_PREFIX.
    Example agency folder: "agency=wholesales/"
    Returns folder names without trailing slash, e.g. "agency=wholesales"
    """
    paginator = s3.get_paginator("list_objects_v2")
    agencies: List[str] = []

    for page in paginator.paginate(Bucket=BUCKET, Prefix=BASE_PREFIX, Delimiter="/"):
        for cp in page.get("CommonPrefixes", []):
            # cp["Prefix"] like ".../agency=wholesales/"
            prefix = cp["Prefix"].rstrip("/")
            agencies.append(prefix.split("/")[-1])

    return agencies


def read_csvs(prefix: str) -> List[str]:
    """
    Reads all .csv files under the given prefix and returns their CSV rows as strings.
    NOTE: If files are big, consider summarizing instead of returning all rows.
    """
    paginator = s3.get_paginator("list_objects_v2")
    output: List[str] = []

    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
        for obj in page.get("Contents", []):
            key = obj["Key"]
            if key.lower().endswith(".csv"):
                print(f"[INFO] Found CSV: {key}")
                response = s3.get_object(Bucket=BUCKET, Key=key)
                body = response["Body"].read().decode("utf-8", errors="replace")
                reader = csv.reader(io.StringIO(body))
                for row in reader:
                    output.append(", ".join(row))

    return output


def publish_sns(subject: str, message: str):
    sns.publish(
        TopicArn=SNS_TOPIC_ARN,
        Subject=subject,
        Message=message[:SNS_MAX]
    )


def publish_sns_chunked(subject: str, message: str):
    """
    Split into multiple SNS messages if payload is too large.
    """
    if len(message) <= SNS_MAX:
        publish_sns(subject, message)
        return

    parts = []
    start = 0
    while start < len(message):
        end = min(start + SNS_MAX, len(message))
        parts.append(message[start:end])
        start = end

    for i, chunk in enumerate(parts, start=1):
        sns.publish(
            TopicArn=SNS_TOPIC_ARN,
            Subject=f"{subject} (part {i}/{len(parts)})",
            Message=chunk
        )
